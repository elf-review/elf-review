import boto3
import shutil
from pathlib import Path
import pandas as pd
from evaluate import evaluator
import datasets
from huggingface_hub import RepoCard
import pickle
from transformers import pipeline
import evaluate
from huggingface_hub.hf_api import repo_info
from transformers.pipelines.conversational import Conversation
from pprint import pprint
import numpy as np
import shutil
import os
import subprocess
import torch
import logging
import json
from huggingface_hub import login
from datasets import load_dataset
import random
from huggingface_hub.utils._errors import RepositoryNotFoundError, EntryNotFoundError
from huggingface_hub.utils._validators import HFValidationError
from botocore.exceptions import ClientError
from transformers import BertTokenizer
from typing import Callable, Any, Iterable
from functools import partial
import ray
from tqdm import tqdm
from munch import DefaultMunch #converts dicts to attribute based objects
import time

# unicode sequences are randomly generated from different intervals of unicode range
# each interval defines a category of symbols or letters that can be represented by it
def random_nonascii_unicode_summarization(alphabet, word_len=30, seq_len=300):
    """returns a randomly generated sequence of unicde

    Args:
        alphabet (list): contains the intervals of unicode
        word_len (int, optional): length of each word. Defaults to 30.
        seq_len (int, optional): length of each sequence or sentence. Defaults to 300. For summarization the sequence length should be
                                long but make sure to truncate or split it based on the max_input_size of llm

    Returns:
        string: unicode sequence
    """
    unicode_seq = ''
    for i in range(0, random.randint(seq_len//10, seq_len)):
        unicode_seq += ' ' + ''.join(random.choice(alphabet) for j in range(random.randint(1, word_len)))
    return unicode_seq

def random_nonascii_unicode_translation(alphabet, word_len=30, seq_len=100):
    """returns a randomly generated sequence of unicde

    Args:
        alphabet (list): contains the intervals of unicode
        word_len (int, optional): length of each word. Defaults to 30.
        seq_len (int, optional): length of each sequence or sentence. Defaults to 100.

    Returns:
        string: unicode sequence
    """
    unicode_seq = ''
    for i in range(0, random.randint(seq_len//10, seq_len)):
        unicode_seq += ' ' + ''.join(random.choice(alphabet) for j in range(random.randint(1, word_len)))
    return unicode_seq

def random_nonascii_unicode_mask(alphabet, word_len=30, seq_len=50):
    """returns a randomly generated sequence of unicde. It also includes word [MASK] which is predicted by llm
    each [MASK] predicted by llm will have a string along with the confidence score. the randomly inserts 
    keyword [MASK] with a probability of 10% after each word. No gurantee that the sequence generated by it will contain 
    keyword [MASK]. Make sure to handle it where you call this fucntion

    Args:
        alphabet (list): contains the intervals of unicode
        word_len (int, optional): length of each word. Defaults to 30.
        seq_len (int, optional): length of each sequence or sentence. Defaults to 50.

    Returns:
        string: unicode sequence
    """
    unicode_seq = ''
    for i in range(0, random.randint(seq_len//10, seq_len)):
        unicode_seq += ' ' + ''.join(random.choice(alphabet) for j in range(random.randint(1, word_len)))
        if random.random() <= 0.10:
            unicode_seq += ' [MASK]'
    return unicode_seq



def download_model(model_path):
    """downloads the compressed model from s3 if present

    Args:
        model_path (str): name and path of the model in s3 bucket

    Returns:
        None or int: int (-1) if it fails otherwise None
    """    

    ml_s3 = boto3.resource(
    service_name="s3",
    region_name="",
    aws_access_key_id="",
    aws_secret_access_key="")
    hf_bucket =  ml_s3.Bucket("")

    try:
        loc_org_file = Path(model_path.local_model_path)
        loc_comp_file = Path(model_path.local_comp_model_path)
        if not loc_comp_file.exists() or hf_bucket.Object(model_path.s3_comp_model_path).content_length != loc_comp_file.stat().st_size:
            hf_bucket.download_file(model_path.s3_comp_model_path, model_path.local_comp_model_path)
        if not loc_org_file.exists() or hf_bucket.Object(model_path.s3_org_model_path).content_length != loc_org_file.stat().st_size:
            hf_bucket.download_file(model_path.s3_org_model_path, model_path.local_model_path)
    except ClientError:
        print('{} - compressed model does not exist'.format(model_path.local_model_path))
        return -1 
    return 

def task_audio_classification(model_path:dict, samples:Iterable[Any], func_dict_all:dict):
    """given a dict that contains paths of original and compressed model along with samples will perform the audio classification on
    them and writes the results to a pickle files.
    Calculate the scores if avilable.
    
    Attributes being written to the file:
        - Input
        - Output from original model (label and its confidence score)
        - Ouput from compressed model
        - comparison of output

    Args:
        model_path (dict): containing paths of original and compressed models
        samples (iterable): samples to be evaluated on
        func_dict_all (dict): contains the metadata about the model

    Returns:
        dict: dict containing results
    """    
    if download_model(model_path):
        return
    func_dict = func_dict_all[model_path.model_name]
    org_model_pipeline =  pipeline(model=model_path.org_model_pipeline, task=func_dict['task'])
    comp_model_pipeline =  pipeline(model=model_path.comp_model_pipeline, task=func_dict['task'])

    #changing the model to float (float32) because sometimes the models only works on float32 inputs
    org_model_pipeline.model = org_model_pipeline.model.float()
    comp_model_pipeline.model = comp_model_pipeline.model.float()
    for sample in samples:
        try:
            output = org_model_pipeline(sample)
            comp_output = comp_model_pipeline(sample)
        except RuntimeError:
            sample =  sample.astype(np.float32)
            output = org_model_pipeline(sample)
            comp_output = comp_model_pipeline(sample)
        try:
            output = output.generated_responses()
            comp_output = comp_output.generated_responses()
        except AttributeError:
            all_labels = []
            all_prob   = []
            avg_prob_diff = []
            for i, j in zip(output, comp_output):
                all_labels.append(i['label']==j['label'])
                all_prob.append(i['score']==j['score'])
                avg_prob_diff.append(i['score']-j['score'])
            func_dict['comparison'].append({'labels_comp':sum(all_labels)==len(all_labels), 
                    'prob_comp':sum(all_prob)==len(all_prob), 'avg_prob_diff':np.mean(np.abs(avg_prob_diff))})
        else:
            #if the output is single lable without any probability then the entire output will be compared, which is a string only
            func_dict['comparison'].append(output==comp_output)
            
        func_dict['inputs'].append(sample)
        func_dict['org_output'].append(output)
        func_dict['comp_output'].append(comp_output)
        

    with open('model_results/'+model_path.model_result_path+'_result.pkl', 'wb') as f:
        pickle.dump(func_dict, f)
    os.remove(model_path.local_model_path)
    os.remove(model_path.local_comp_model_path)
    return func_dict


def task_automatic_speech_recognition(model_path:dict, samples:Iterable[Any], func_dict_all:dict):
    """given a dict that contains paths of original and compressed model along with samples will perform the automatic speech recogniton task on
    them and writes the results to a pickle file 
    
    Attributes being written to the file:
        - Input
        - Output from original model (a sequence of string/text)
        - Ouput from compressed model
        - comparison of output

    Args:
        model_path (dict): containing paths of original and compressed models
        samples (iterable): samples to be evaluated on
        func_dict_all (dict): contains the metadata about the model

    Returns:
        dict: dict containing results
    """
    if download_model(model_path):
        return

    func_dict = func_dict_all[model_path.model_name]
    org_model_pipeline =  pipeline(model=model_path.org_model_pipeline, task=func_dict['task'])
    comp_model_pipeline =   pipeline(model=model_path.comp_model_pipeline, task=func_dict['task'])
    org_model_pipeline = org_model_pipeline.model.float()
    comp_model_pipeline = comp_model_pipeline.model.float()
    for sample in samples:
        output = org_model_pipeline(sample)['text'] #get the text generated by the model using keyword 'text
        comp_output = comp_model_pipeline(sample)['text']
        func_dict['inputs'].append(sample)
        func_dict['org_output'].append(output)
        func_dict['comp_output'].append(comp_output)
        func_dict['comparison'].append(output==comp_output)
    
    with open('model_results/'+model_path.model_result_path+'_result.pkl', 'wb') as f:
        pickle.dump(func_dict, f)
    os.remove(model_path.local_model_path)
    os.remove(model_path.local_comp_model_path)
    return func_dict

def task_conversational(model_path:dict, samples:Iterable[Any], func_dict_all:dict):
    """given a dict that contains paths of original and compressed model along with samples will perform a conversation based task on
    them and writes the results to a pickle files
    Each sample is converted to Conversation object. It is an object provided by huggingface. For conversational task the sample or
        input must be an object of Conversation. The object stores the previous prompts of the user and all the response generated by
        the model. These attributes are used as context.
    Attributes being written to the file:
        - Input
        - Output from original model (bot like response)
        - Ouput from compressed model
        - comparison of output

    Args:
        model_path (dict): containing paths of original and compressed models
        samples (iterable): samples to be evaluated on
        func_dict_all (dict): contains the metadata about the model

    Returns:
        dict: dict containing results
    """
    if download_model(model_path):
        return
    func_dict = func_dict_all[model_path.model_name]

    org_model_pipeline =  pipeline(model=model_path.org_model_pipeline, task=func_dict['task'])
    comp_model_pipeline =   pipeline(model=model_path.comp_model_pipeline, task=func_dict['task'])
    for sample in samples:
        conversation = Conversation()
        conversation.add_user_input(sample)
        output = org_model_pipeline(conversation)
        conversation = Conversation()
        conversation.add_user_input(sample)
        comp_output = comp_model_pipeline(conversation)
        func_dict['inputs'].append(sample)
        func_dict['org_output'].append(output)
        func_dict['comp_output'].append(comp_output)
        sample_comp = []
        for i, j in zip(output.generated_responses, comp_output.generated_responses):
            sample_comp.append(i==j) #each response is compared one to one. Sometimes the model will response with multiple outputs
                                    # it is up to the user which one to select but in this fuzzy testing environment we will compare all the respones
                                    #if one of them is different then the entire comparsion will return false i.e. they are not the same
        func_dict['comparison'].append(sample_comp)
    
    with open('model_results/'+model_path.model_result_path+'_result.pkl', 'wb') as f:
        pickle.dump(func_dict, f)
    os.remove(model_path.local_model_path)
    os.remove(model_path.local_comp_model_path)
    return func_dict
        

def task_feature_extraction(model_path:dict, samples:Iterable[Any], func_dict_all:dict):
    """given a dict that contains paths of original and compressed model along with samples will generate a vector representation of
    them and writes the results to a pickle files
    The generated vectors from original model and compressed models are very different. The is the major task that 
    whose output is almost always differs from the compressed model. Same as the probabilities given by a compressed and orginal model 
    are not same but the labels are.
    I compare all the values of the vectirs and check if their absolute difference is within a tolerence or not. 
    The hard part is to define a reasonable tolerance value itself
    
    Attributes being written to the file:
        - Input
        - Output from original model (vector)
        - Ouput from compressed model
        - comparison of output

    Args:
        model_path (dict): containing paths of original and compressed models
        samples (iterable): samples to be evaluated on
        func_dict_all (dict): contains the metadata about the model

    Returns:
        dict: dict containing results
    """
    if download_model(model_path):
        return
    func_dict = func_dict_all[model_path.model_name]
    org_model_pipeline =  pipeline(model=model_path.org_model_pipeline, task=func_dict['task'])
    comp_model_pipeline =   pipeline(model=model_path.comp_model_pipeline, task=func_dict['task'])
    for sample in samples:
        output = org_model_pipeline(sample)
        comp_output = comp_model_pipeline(sample)
        func_dict['inputs'].append(sample)
        func_dict['org_output'].append(output)
        func_dict['comp_output'].append(comp_output)
        #checks if the values are within different tolerance values
        func_dict['comparison'].append({'output_comp':output==comp_output, 
                                                                    'ouput_close_comp_3':np.allclose(output, comp_output, atol=1e-3, equal_nan=True),
                                                                    'ouput_close_comp_4':np.allclose(output, comp_output, atol=1e-4, equal_nan=True),
                                                                    'ouput_close_comp_5':np.allclose(output, comp_output, atol=1e-5, equal_nan=True),
                                                                    'ouput_close_comp_6':np.allclose(output, comp_output, atol=1e-6, equal_nan=True),
                                                                    'ouput_close_comp_7':np.allclose(output, comp_output, atol=1e-7, equal_nan=True),
                                                                    'ouput_close_comp_8':np.allclose(output, comp_output, atol=1e-8, equal_nan=True),
                                                                    })
    with open('model_results/'+model_path.model_result_path+'_result.pkl', 'wb') as f:
        pickle.dump(func_dict, f)
    os.remove(model_path.local_model_path)
    os.remove(model_path.local_comp_model_path)
    return func_dict
    
def task_fill_mask(model_path:dict, samples:Iterable[Any], func_dict_all:dict):
    """given a dict that contains paths of original and compressed model along with samples will predict the word being masked by keyword
    [MASK] and writes the results to a pickle files
    
    Attributes being written to the file:
        - Input
        - Output from original model (predicted word for each [MASK] occurance along with the confidence score)
        - Ouput from compressed model
        - comparison of output

    Args:
        model_path (dict): containing paths of original and compressed models
        samples (iterable): samples to be evaluated on
        func_dict_all (dict): contains the metadata about the model

    Returns:
        dict: dict containing results
    """
    if download_model(model_path):
        return
    func_dict = func_dict_all[model_path.model_name]
    org_model_pipeline =  pipeline(model=model_path.org_model_pipeline, task=func_dict['task'])
    comp_model_pipeline =   pipeline(model=model_path.comp_model_pipeline, task=func_dict['task'])
    for sample in samples:
        try:
            output = org_model_pipeline(sample)
            comp_output = comp_model_pipeline(sample)
        except TypeError:
            return

        for i, j in zip(output, comp_output):
            all_id_token  = []
            all_str_token = []
            all_score     = []
            score_diff    = []
            try:
                #loop through each [MASK] keyword if model gives multiple values for each mask
                for org_result, comp_result in zip(i, j):
                    all_id_token.append(org_result['token']==comp_result['token'])
                    all_str_token.append(org_result['token_str']==comp_result['token_str'])
                    all_score.append(org_result['score']==comp_result['score'])
                    score_diff.append(org_result['score']-comp_result['score'])
                    del org_result['sequence'], comp_result['sequence']
            except TypeError:
                    #loop if there is only 1 prediction for each [MASK] keyword
                    for i, j in zip(output, comp_output):
                        all_id_token.append(i['token']==j['token'])
                        all_str_token.append(i['token_str']==j['token_str'])
                        all_score.append(i['score']==j['score'])
                        score_diff.append(i['score']-j['score'])
                        del i['sequence'], j['sequence']
                    break
        
        func_dict['inputs'].append(sample)
        func_dict['org_output'].append(output)
        func_dict['comp_output'].append(comp_output)
        try:
            # if there is only one [MASK] keyword and only one prediction for the keyword
            func_dict['comparison'].append({'token_id_comp':sum(all_id_token)==len(all_id_token), 
                        'token_str_comp':sum(all_str_token)==len(all_str_token), 
                        'prob_comp':sum(all_score)==len(all_score), 'avg_prob_diff':np.mean(np.abs(score_diff))})
                
        except UnboundLocalError:
                func_dict['comparison'].append({'token_id_comp':output==comp_output, 
                        'token_str_comp':output==comp_output, 
                        'prob_comp':output==comp_output, 'avg_prob_diff':0})
                
        else:
            del all_id_token, all_str_token, all_score, score_diff
        
    with open('model_results/'+model_path.model_result_path+'_result.pkl', 'wb') as f:
        pickle.dump(func_dict, f)
    os.remove(model_path.local_model_path)
    os.remove(model_path.local_comp_model_path)
    return func_dict
        
     
def task_image_classification(model_path:dict, samples:Iterable[Any], func_dict_all:dict):
    """given a dict that contains paths of original and compressed model along with samples will perform the image classification task on
    them and writes the results to a pickle files
    
    Attributes being written to the file:
        - Input
        - Output from original model (labels and confidence score)
        - Ouput from compressed model
        - comparison of output

    Args:
        model_path (dict): containing paths of original and compressed models
        samples (iterable): samples to be evaluated on
        func_dict_all (dict): contains the metadata about the model

    Returns:
        dict: dict containing results
    """
    if download_model(model_path):
        return
    func_dict = func_dict_all[model_path.model_name]
    org_model_pipeline =  pipeline(model=model_path.org_model_pipeline, task=func_dict['task'])
    comp_model_pipeline =   pipeline(model=model_path.comp_model_pipeline, task=func_dict['task'])
    for sample in samples:
        output = org_model_pipeline(sample)
        comp_output = comp_model_pipeline(sample)
        func_dict['inputs'].append(sample)
        func_dict['org_output'].append(output)
        func_dict['comp_output'].append(comp_output)
        all_labels = []
        all_prob   = []
        avg_prob_diff = []
        for i, j in zip(output, comp_output):
            all_labels.append(i['label']==j['label'])
            all_prob.append(i['score']==j['score'])
            avg_prob_diff.append(i['score']-j['score'])
        func_dict['comparison'].append({'labels_comp':sum(all_labels)==len(all_labels), 
                    'prob_comp':sum(all_prob)==len(all_prob), 'avg_prob_diff':np.mean(np.abs(avg_prob_diff))})
    os.remove(model_path.local_model_path)
    os.remove(model_path.local_comp_model_path)
    with open('model_results/'+model_path.model_result_path+'_result.pkl', 'wb') as f:
        pickle.dump(func_dict, f)
    return func_dict

def task_question_answering(model_path:dict, samples:Iterable[Any], func_dict_all:dict):
    """given a dict that contains paths of original and compressed model along with samples (as questions) will try to
      answer the given question and writes the results to a pickle files
    
    Attributes being written to the file:
        - Input
        - Output from original model (answer (str) along with start: index in the question, end: index in question
                answer is usually a keyword in question[start:end] but not necessarily)
        - Ouput from compressed model
        - comparison of output

    Args:
        model_path (dict): containing paths of original and compressed models
        samples (iterable): samples to be evaluated on
        func_dict_all (dict): contains the metadata about the model

    Returns:
        dict: dict containing results
    """
    if download_model(model_path):
        return
    func_dict = func_dict_all[model_path.model_name]
    org_model_pipeline =  pipeline(model=model_path.org_model_pipeline, task=func_dict['task'])
    comp_model_pipeline =   pipeline(model=model_path.comp_model_pipeline, task=func_dict['task'])
    for sample in samples:
        output = org_model_pipeline(sample)
        comp_output = comp_model_pipeline(sample)
        comp_dict = output==comp_output
        func_dict['inputs'].append(sample)
        func_dict['org_output'].append(output)
        func_dict['comp_output'].append(comp_output)
        func_dict['comparison'].append(comp_dict)

        func_dict['comparison'].append({'answers_comp':output['answer']==comp_output['answer'], 
                    'start_comp':output['start']==comp_output['start'], 'end_comp':output['end']==comp_output['end'],
                    'prob_comparison': output['score']==comp_output['score']})

    with open('model_results/'+model_path.model_result_path+'_result.pkl', 'wb') as f:
        pickle.dump(func_dict, f)
    os.remove(model_path.local_model_path)
    os.remove(model_path.local_comp_model_path)
    return func_dict

def task_summarization(model_path:dict, samples:Iterable[Any], func_dict_all:dict):
    """given a dict that contains paths of original and compressed model along with samples will summarize
    them and writes the results to a pickle files
    
    Attributes being written to the file:
        - Input
        - Output from original model (sequence of text)
        - Ouput from compressed model
        - comparison of output

    Args:
        model_path (dict): containing paths of original and compressed models
        samples (iterable): samples to be evaluated on
        func_dict_all (dict): contains the metadata about the model

    Returns:
        dict: dict containing results
    """
    if download_model(model_path):
        return
    func_dict = func_dict_all[model_path.model_name]
    org_model_pipeline =  pipeline(model=model_path.org_model_pipeline, task=func_dict['task'])
    comp_model_pipeline =   pipeline(model=model_path.comp_model_pipeline, task=func_dict['task'])
    for sample in samples:
        tokenizer_kwargs = {'truncation':True}
        # if the models does not have predefined tokenzier then bert will be used as a default tokenizer
        # this should not affect the comparison because both original and compressed model will not have a tokenizer and
            # they will be used with the same tokenizer so this should not change their output as backend tokenizer of both models is same
        if org_model_pipeline.tokenizer:
            pass
        else: 
            org_model_pipeline.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

        if comp_model_pipeline.tokenizer:
            pass
        else: 
            comp_model_pipeline.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

        output = org_model_pipeline(sample, **tokenizer_kwargs)[0]['summary_text']
        comp_output = comp_model_pipeline(sample, **tokenizer_kwargs)[0]['summary_text']
        func_dict['inputs'].append(sample)
        func_dict['org_output'].append(output)
        func_dict['comp_output'].append(comp_output)
        func_dict['comparison'].append(output==comp_output)

    with open('model_results/'+model_path.model_result_path+'_result.pkl', 'wb') as f:
        pickle.dump(func_dict, f)
    os.remove(model_path.local_model_path)
    os.remove(model_path.local_comp_model_path)
    return func_dict

def task_text_generation(model_path:dict, samples:Iterable[Any], func_dict_all:dict):
    """given a dict that contains paths of original and compressed model along with samples will try to comple the sentences 
        and writes the results to a pickle files
    
    Attributes being written to the file:
        - Input
        - Output from original model 
        - Ouput from compressed model
        - comparison of output

    Args:
        model_path (dict): containing paths of original and compressed models
        samples (iterable): samples to be evaluated on
        func_dict_all (dict): contains the metadata about the model

    Returns:
        dict: dict containing results
    """
    if download_model(model_path):
        return
    func_dict = func_dict_all[model_path.model_name]
    org_model_pipeline =  pipeline(model=model_path.org_model_pipeline, task=func_dict['task'])
    comp_model_pipeline =   pipeline(model=model_path.comp_model_pipeline, task=func_dict['task'])
    for sample in samples:
        output = org_model_pipeline(sample, max_length=30)
        comp_output = comp_model_pipeline(sample, max_length=30)
        try:
            output = output[0]['generated_text']
            comp_output = comp_output[0]['generated_text']

            if '\n' in output:
                output = output.split('\n')[0]
            if '\n' in comp_output:
                comp_output = comp_output.split('\n')[0]
        except KeyError:
            pass
     
        bleu = evaluate.load("bleu")
        org_bleu_results = bleu.compute(predictions=[sample], references=[output])['bleu']
        comp_bleu_results = bleu.compute(predictions=[sample], references=[comp_output])['bleu']
        print(org_bleu_results, comp_bleu_results)
        func_dict['inputs'].append(sample)
        func_dict['org_output'].append(output)
        func_dict['comp_output'].append(comp_output)
        func_dict['comparison'].append({'exact_comp':output==comp_output, 
                                        'bleu_score_comp': org_bleu_results<=comp_bleu_results,
                                         'bleu_score_abs_diff': abs(org_bleu_results-comp_bleu_results) })

    with open('model_results/'+model_path.model_result_path+'_result.pkl', 'wb') as f:
        pickle.dump(func_dict, f)
    os.remove(model_path.local_model_path)
    os.remove(model_path.local_comp_model_path)
    return func_dict

def task_text2text_generation(model_path:dict, samples:Iterable[Any], func_dict_all:dict):
    """given a dict that contains paths of original and compressed model along with samples will either complete the text, 
    summarize or translate them and writes the results to a pickle files
    
    Attributes being written to the file:
        - Input
        - Output from original model (sequence of string)
        - Ouput from compressed model
        - comparison of output

    Args:
        model_path (dict): containing paths of original and compressed models
        samples (iterable): samples to be evaluated on
        func_dict_all (dict): contains the metadata about the model

    Returns:
        dict: dict containing results
    """
    if download_model(model_path):
        return
    
    func_dict = func_dict_all[model_path.model_name]
    org_model_pipeline =  pipeline(model=model_path.org_model_pipeline, task=func_dict['task'])
    comp_model_pipeline =   pipeline(model=model_path.comp_model_pipeline, task=func_dict['task'])

    for sample in samples:
        tokenizer_kwargs = {'truncation':True, 'max_length':20}
        if org_model_pipeline.tokenizer:
            pass
        else: 
            org_model_pipeline.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

        if comp_model_pipeline.tokenizer:
            pass
        else: 
            comp_model_pipeline.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    for sample in samples:
        output = org_model_pipeline(sample, **tokenizer_kwargs)
        comp_output = comp_model_pipeline(sample, **tokenizer_kwargs)
        func_dict['inputs'].append(sample)
        func_dict['org_output'].append(output)
        func_dict['comp_output'].append(comp_output)
        func_dict['comparison'].append(output==comp_output)
    with open('model_results/'+model_path.model_result_path+'_result.pkl', 'wb') as f:
        pickle.dump(func_dict, f)
    os.remove(model_path.local_model_path)
    os.remove(model_path.local_comp_model_path)
    return func_dict

def task_token_classification(model_path:dict, samples:Iterable[Any], func_dict_all:dict):
    """given a dict that contains paths of original and compressed model along with samples will perform classification on
    every word in the sequence and writes the results to a pickle files
    
    Attributes being written to the file:
        - Input
        - Output from original model 
        - Ouput from compressed model
        - comparison of output

    Args:
        model_path (dict): containing paths of original and compressed models
        samples (iterable): samples to be evaluated on
        func_dict_all (dict): contains the metadata about the model

    Returns:
        dict: dict containing results
    """
    if download_model(model_path):
        return
    func_dict = func_dict_all[model_path.model_name]
    org_model_pipeline =  pipeline(model=model_path.org_model_pipeline, task=func_dict['task'])
    comp_model_pipeline =   pipeline(model=model_path.comp_model_pipeline, task=func_dict['task'])
    tokenizer_kwargs = {'max_length':512}
    for sample in samples:
        output = org_model_pipeline(sample)
        comp_output = comp_model_pipeline(sample)
        #print(output)
        #print(comp_output)
        
        for i, j in zip(output, comp_output):
            all_id_token  = []
            all_str_token = []
            all_score     = []
            all_start = []
            all_end  = []
            all_word = []
            score_diff    = []
            try:
                for org_result, comp_result in zip(i, j):
                    all_id_token.append(org_result['entity']==comp_result['entity'])
                    all_str_token.append(org_result['index']==comp_result['index'])
                    all_start.append(org_result['start']==comp_result['start'])
                    all_end.append(org_result['end']==comp_result['end'])
                    all_word.append(org_result['word']==comp_result['word'])
                    all_score.append(org_result['score']==comp_result['score'])
                    score_diff.append(org_result['score']-comp_result['score'])
            except TypeError:
                #no need for loop if there is only one token
                all_id_token.append(i['entity']==j['entity'])
                all_str_token.append(i['index']==j['index'])
                all_start.append(i['start']==j['start'])
                all_end.append(i['end']==j['end'])
                all_word.append(i['word']==j['word'])
                all_score.append(i['score']==j['score'])
                score_diff.append(i['score']-j['score'])
            break

        func_dict['inputs'].append(sample)
        func_dict['org_output'].append(output)
        func_dict['comp_output'].append(comp_output)
        try:
            #will return true if the classification of each token is true
            func_dict['comparison'].append({'token_id_comp':sum(all_id_token)==len(all_id_token), 
                    'token_str_comp':sum(all_str_token)==len(all_str_token), 
                    'prob_comp':sum(all_score)==len(all_score), 'avg_prob_diff':np.mean(np.abs(score_diff)),
                    'all_start_comp':sum(all_start)==len(all_start),
                    'all_score_comp':sum(all_score)==len(all_score),
                    'all_end_comp':sum(all_end)==len(all_end),
                    'all_word_comp':sum(all_word)==len(all_word)})
            
        
        except UnboundLocalError:
            func_dict['comparison'].append({'token_id_comp':output==comp_output, 
                    'token_str_comp':output==comp_output, 
                    'prob_comp':output==comp_output, 'avg_prob_diff':0,
                    'all_start_comp':output==comp_output,
                    'all_score_comp':output==comp_output,
                    'all_end_comp':output==comp_output,
                    'all_word_comp':output==comp_output})
        else:
            del all_id_token, all_str_token,all_score, all_start, all_end, all_word, score_diff
    with open('model_results/'+model_path.model_result_path+'_result.pkl', 'wb') as f:
        pickle.dump(func_dict, f)
    os.remove(model_path.local_model_path)
    os.remove(model_path.local_comp_model_path)
    return func_dict
        

def task_translation(model_path:dict, samples:Iterable[Any], func_dict_all:dict):
    """given a dict that contains paths of original and compressed model along with samples will translate them
    to a specific language and writes the results to a pickle files
    
    Attributes being written to the file:
        - Input
        - Output from original model (a sequence of string)
        - Ouput from compressed model
        - comparison of output

    Args:
        model_path (dict): containing paths of original and compressed models
        samples (iterable): samples to be evaluated on
        func_dict_all (dict): contains the metadata about the model

    Returns:
        dict: dict containing results
    """
    if download_model(model_path):
        return
    func_dict = func_dict_all[model_path.model_name]
    try:
        org_model_pipeline =  pipeline(model=model_path.org_model_pipeline, task=func_dict['task'])
        comp_model_pipeline =   pipeline(model=model_path.comp_model_pipeline, task=func_dict['task'])
        for sample in samples:
            output = org_model_pipeline(sample)[0]['translation_text']
            comp_output = comp_model_pipeline(sample)[0]['translation_text']
            func_dict['inputs'].append(sample)
            func_dict['org_output'].append(output)
            func_dict['comp_output'].append(comp_output)
            func_dict['comparison'].append(output==comp_output)
    except RuntimeError:
        return
    with open('model_results/'+model_path.model_result_path+'_result.pkl', 'wb') as f:
        pickle.dump(func_dict, f)
    os.remove(model_path.local_model_path)
    os.remove(model_path.local_comp_model_path)
    return func_dict


def task_text_classification(model_path:dict, samples:Iterable[Any], func_dict_all:dict):
    """given a dict that contains paths of original and compressed model along with samples will perform the task classification on
    the entire text and writes the results to a pickle files
    
    Attributes being written to the file:
        - Input
        - Output from original model (label with confidence score)
        - Ouput from compressed model
        - comparison of output

    Args:
        model_path (dict): containing paths of original and compressed models
        samples (iterable): samples to be evaluated on
        func_dict_all (dict): contains the metadata about the model

    Returns:
        dict: dict containing results
    """
    if download_model(model_path):
        return
    func_dict = func_dict_all[model_path.model_name]
    org_model_pipeline =  pipeline(model=model_path.org_model_pipeline, task=func_dict['task'])
    comp_model_pipeline =   pipeline(model=model_path.comp_model_pipeline, task=func_dict['task'])
    tokenizer_kwargs = {'truncation':True, 'max_length':512}
    for sample in samples:
        output = org_model_pipeline(sample,**tokenizer_kwargs)
        comp_output = comp_model_pipeline(sample, **tokenizer_kwargs)
        func_dict['inputs'].append(sample)
        func_dict['org_output'].append(output)
        func_dict['comp_output'].append(output==comp_output)
        all_labels = []
        all_prob = []
        avg_prob_diff = []
        for i, j in zip(output, comp_output):
                all_labels.append(i['label']==j['label'])
                all_prob.append(i['score']==j['score'])
                avg_prob_diff.append(i['score']-j['score'])
                func_dict['comparison'].append({'labels_comp':sum(all_labels)==len(all_labels), 
                        'prob_comp':sum(all_prob)==len(all_prob), 'avg_prob_diff':np.mean(np.abs(avg_prob_diff))})
    with open('model_results/'+model_path.model_result_path+'_result.pkl', 'wb') as f:
        pickle.dump(func_dict, f)
    os.remove(model_path.local_model_path)
    os.remove(model_path.local_comp_model_path)
    return func_dict

#print('sleeping')
#time.sleep(1200)
# sets up the credentials to access both compressed and original models on s3
ml_s3 = boto3.resource(
    service_name="s3",
    region_name="us-east-1",
    aws_access_key_id="AKIAS7X5VN6VXEVJVCN2",
    aws_secret_access_key="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"
)

#logging file
logging.basicConfig(filename = 'fuzzy_testing_600.log',
                    level = logging.ERROR,
                    format = '%(asctime)s:%(levelname)s:%(name)s:%(message)s')

# huggingface auth token
auth_token = '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'
login(auth_token)

# sets up the device. it will only use 1 gpu if its available
device = "cuda:0" if torch.cuda.is_available() else "cpu"
print('using device', device)
d = dict(os.environ)   # Make a copy of the current environment
d['GIT_LFS_SKIP_SMUDGE'] = '1'

# reads the unicode intervals from a csv file.
character_ranges = pd.read_csv('convert_ascii_clean.csv', sep=',')
character_ranges = character_ranges.loc[character_ranges['type']!='0']
include_ranges = [(int(i,16), int(j,16)) for i, j in zip(character_ranges['start'], character_ranges['end'])]
alphabet = [
        chr(code_point) for current_range in include_ranges
            for code_point in range(current_range[0], current_range[1] + 1)
    ]

# gets the models stored in the s3 bucket
hf_bucket =  ml_s3.Bucket("ml-model-storage-compression")
hf_model_list = ml_s3.Bucket("ml-model-storage-compression").objects.filter(Prefix = "model_original")
hf_models = [tf_item for tf_item in hf_model_list.all() if tf_item.key.endswith("pytorch_model.bin")]

#loads model from the csv file. These models will be used for comparison
print('loading models from csv...')
df = pd.read_csv('models_repo_info_fuzzy_600.csv')

#loads different datasets for each task. The datasets are downloaded from huggingface.
# The datasets are downloaded locally to cache folder of huggingface are loaded locally if they are already exist in the cache folder
print('loading data...')
audio_classification_dataset = datasets.load_dataset("Aniemore/resd")
automatic_speech_recognitiondataset = load_dataset("arabic_speech_corpus")
conversational_dataset = load_dataset('blended_skill_talk')
#subsample size for large datasets
#string=True means that the dataset will not be downloaded and only the samples requested will be downloaded
subsample_size = 1000
imagenet_dataset = load_dataset("imagenet-1k", streaming=True, split='validation')
shuffled_dataset = imagenet_dataset.shuffle(seed=444, buffer_size=10000) # buffer size is the first x sample that will be used to shuffle 
imagenet_list_dataset = list(shuffled_dataset.take(subsample_size)) #list() will download the requested samples from the dataset
#take attribute (only when straming=True) will select the samples but will only download them if iterated over it or list() is being called on it
#split defines the subset to be used
#configuration or second argument of load_dataset will download the specific version of the dataset
dataset_qa = load_dataset('xtreme', 'MLQA.de.en' ,split='validation', streaming=True)
dataset_qa = list(dataset_qa.take(1000))
dataset_qa_mcq = load_dataset("openbookqa", split='test')
text_gen_dataset = load_dataset("alespalla/chatbot_instruction_prompts", split='train')

blacklisted_models = ['AlexN-xls-r-300m-pt', 'Ayoola-cdial-yoruba-test']

#number of samples to be selected for fuzzy testing
sample_numbers = 10
function_array = [None for i in range(df.shape[0])]
total_item = 0

print('creating input samples for parallel fuzzy testing...')
for model_idx, model_item in tqdm(df.iterrows(), total=df.shape[0]): #iterates over the dataframe
    
    # dict to hold the values after fuzzy testing
    output_dict = {}
    output_dict[model_item['model_name']]={}
    output_dict[model_item['model_name']]['idx'] = model_idx
    output_dict[model_item['model_name']]['sample_num'] = sample_numbers
    output_dict[model_item['model_name']]['inputs'] = []
    output_dict[model_item['model_name']]['org_output'] = []
    output_dict[model_item['model_name']]['comp_output'] = []
    output_dict[model_item['model_name']]['comparison'] = []
    output_dict[model_item['model_name']]['task'] = model_item['category']
    output_dict[model_item['model_name']]['name'] = model_item['model_name']
    output_dict[model_item['model_name']]['repo'] = model_item['repo_link'] #repo_link of model on huggingface
    local_model_path = 'models_comp/'+model_item['repo_link']+'/pytorch_model.bin'
    local_comp_model_path = 'models_compressed/'+model_item['repo_link']+'/pytorch_model.bin'

    #dict containing the paths of models stored locally or on s3
    #models are downloaded locally from the s3 bucket because the metadata is stored locally and huggingface pipeline
        #requires the model and metadata to be stored in the same path
    model_paths = {'comp_model_pipeline':'models_compressed/'+model_item['repo_link'],
                    'org_model_pipeline':'models_comp/'+model_item['repo_link'],
                    'local_model_path':local_model_path,
                    'local_comp_model_path':local_comp_model_path, 
                    's3_org_model_path':model_item['original_model_path'],
                    's3_comp_model_path':model_item['compressed_model_path'],
                    'model_result_path': model_item['model_name'],
                     'model_name':model_item['model_name'] }
    
    model_paths = DefaultMunch.fromDict(model_paths)

    
    if model_item['model_name'] in blacklisted_models:
        continue
    #ignore loading model if the result/pickle file already exists or if the compressed model is not in the s3 bucket
    try: 
        c = hf_bucket.Object(model_item['compressed_model_path']).content_length
    except ClientError:
        continue

    #counts the number of models in the list that are to be used for fuzzy testing
    #the variable limits the number of models that are processed in parallel
    total_item += 1

    #will perform the fuzzy testing based on the task of the model defined in the pipeline
    #the information of the task is downloaded through huggingface api and this code assumes that the file containing all the
        #metadata is downloaded and available
    match model_item['category']:
        case 'audio-classification':
            pass
        case 'automatic-speech-recognition':
            pass
        case 'conversational':
            pass
        case 'feature-extraction':
            pass
        case 'fill-mask':
            pass
        case 'image-classification':
            pass
        case 'question-answering':
            pass
        case 'summarization':
            pass
        case 'text-classification':
            pass
        case 'text-generation':
            samples = []
            for i in range(sample_numbers):
                while True:
                    sample_id = random.randint(0, len(text_gen_dataset)-1)
                    prompt_words = text_gen_dataset[sample_id]['prompt'].split(' ')
                    if len(prompt_words)>3:
                        break
                prompt_len   = random.randint(2, len(prompt_words)-1)
                sample = ' '.join(prompt_words[0:prompt_len])
                samples.append(sample)
            function_array[model_idx] = [partial(ray.remote(task_text_generation).remote, model_paths, samples, output_dict), output_dict]
        case 'text2text-generation':
            pass
        case 'token-classification':
            pass
        case 'translation':
           pass
        case 'image-segmentation':
            pass
        case 'image-to-text':
            pass
        case 'universal-dependencies':
            pass
        case 'zero-shot-classification':
            pass
        case 'zero-shot-image-classification':
            pass
        case 'multiple-choice':
            pass

#MAX_NUM_PENDING_TASKS = 30
result_refs = []
ray.init(num_cpus=30)

#using ray to process models in parallel
def to_iterator(obj_ids):
    while obj_ids:
        try:
            done, obj_ids = ray.wait(obj_ids)
            yield ray.get(done[0])
        except: #will ignore all the errors raised by the model or pipeline
            yield None


tasks = [i[0]() for i in function_array if i]
#uses tqdm to keep track of the progress
results = [result for result in tqdm(to_iterator(tasks), total=len(tasks))]
results = [result for result in results if results]

with open('all_fuzzy_results.pkl', 'wb') as f:
    pickle.dump(results, f)
   